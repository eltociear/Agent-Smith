"""
This script is to craft adversarial images using border attack / pixel attack
"""
import os
import csv
import torch
import random
import argparse
import torchvision
import numpy as np
from PIL import Image
from tqdm import tqdm
from accelerate import Accelerator
from diff_jpeg import diff_jpeg_coding
from torchvision.utils import save_image
from torch.utils.data import Dataset, DataLoader
from transformers import AutoProcessor, LlavaForConditionalGeneration, CLIPProcessor, CLIPModel


def parse_args():
    parser = argparse.ArgumentParser()

    # parameters related to attack
    parser.add_argument("--attacker", type=str, default='MI', help="attack method")
    parser.add_argument("--pixel_attack", action='store_true', default=False, help="whether to enable pixel attack")
    parser.add_argument("--border", type=int, default=6, help="border width for border attack")
    parser.add_argument("--size", type=int, default=336, help="image size of adversarial examples")
    parser.add_argument("--epochs", type=int, default=100, help="number of optimization epochs")
    parser.add_argument("--epsilon", type=int, default=8, help="perturbation budget for pixel attack")
    parser.add_argument("--eta", type=float, default=0.5, help="step size")
    parser.add_argument("--seed", type=int, default=42, help="random seed")
    parser.add_argument("--mu", type=float, default=0.95, help="momentum factor for vlm attack")
    parser.add_argument("--lambda_r", type=float, default=1.0, help="RAG loss weight")
    parser.add_argument("--unconstrained", action='store_true', default=False, help="whether to enable unconstrained optimization, true for border attack")
    
    parser.add_argument("--root", type=str, default="expeirments", help="folder to save results")
    parser.add_argument("--div", type=str, default="high", help="scenario for textual chat diversity, low or high")
    parser.add_argument("--dtype", type=str, default="bf16", help="data type for models and data, bf16 or fp16 or fp32")
    parser.add_argument("--ensemble_size", type=int, default=512, help="ensemble sample size")
    parser.add_argument("--save_epoch", type=int, default=1, help="per epoch for saving adversarial images")
    parser.add_argument("--resume", action='store_true', default=False, help="whether to resume optimization")
    parser.add_argument("--valid_epoch", type=int, default=10, help="per epoch for validation")

    # parameters related to input augmentation
    parser.add_argument("--prob_random_flip", type=float, default=0.0, help="probability of enabling random flip during the optimization")
    parser.add_argument("--enable_random_resize", action='store_true', default=False, help="whether to enable random resize during the optimization")
    parser.add_argument("--upper_random_resize", type=int, default=448, help="upper image resize for random resize")
    parser.add_argument("--lower_random_resize", type=int, default=224, help="lower image resize for random resize")
    parser.add_argument("--prob_random_jpeg", type=float, default=0.0, help="probability of enabling random jpeg during the optimization")

    # parameters related to models
    parser.add_argument("--vlm", type=str, default='llava-hf/llava-1.5-7b-hf', help="vlm model path")
    parser.add_argument("--rag", type=str, default='openai/clip-vit-large-patch14', help="rag model path")
    parser.add_argument("--max_new_tokens", type=int, default=64, help="maximum of new tokens generated by vlm")
    parser.add_argument("--do_sample", action='store_true', default=False)
    args = parser.parse_args()
    return args


def set_seeds(seed):    
    random.seed(seed)
    np.random.seed(seed)
    
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def clamp(x, ori_x, epsilon, norm='Linf'):
    B = x.shape[0]
    if norm == 'Linf':
        x = torch.clamp(x, min=ori_x - epsilon, max=ori_x + epsilon)
    elif norm == 'L2':
        difference = x - ori_x
        distance = torch.norm(difference.view(B, -1), p=2, dim=1)
        mask = distance > epsilon
        if torch.sum(mask) > 0:
            difference[mask] = difference[mask] / distance[mask].view(torch.sum(mask), 1, 1, 1) * epsilon
            x = ori_x + difference
    x = torch.clamp(x, min=0, max=1)
    return x


class ProcessorGradientFlow():
    """
    This wraps up the huggingface CLIP processor to allow backprop through the image processing step.
    The original processor forces conversion to numpy then PIL images, which is faster for image processing but breaks gradient flow. 
    """
    def __init__(self, processor):
        self.processor = processor
        self.image_mean = processor.image_mean
        self.image_std = processor.image_std
        self.normalize = torchvision.transforms.Normalize(
            self.image_mean,
            self.image_std
        )
        height, width = processor.crop_size['height'], processor.crop_size['width']
        assert height == width
        assert height == processor.size['shortest_edge']
        self.resize = torchvision.transforms.Resize(height, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True)
        self.center_crop = torchvision.transforms.CenterCrop(height)

    def preprocess_img(self, images):
        images = self.resize(images)
        images = images.clamp(0, 1)
        images = self.center_crop(images)
        images = self.normalize(images)
        return images


# MI-FGSM Attack
def AttackMI(args, 
             ori_image,
             resume_epoch,
             resume_momentum,
             accelerator, 
             model, 
             train_dataloader,
             save_dir,
             train_log
            ):
    
    # initialize hyper-parameters
    epsilon = args.epsilon / 255
    mu = args.mu
    eta = args.eta / 255
    epochs = args.epochs
    lambda_r = args.lambda_r
    unconstrained = args.unconstrained
    border = args.border
    img_size = args.size
    pixel_attack = args.pixel_attack

    B, C, H, W = 1, 3, img_size, img_size
    
    max_train_steps = int(np.ceil(args.ensemble_size * 2 / accelerator.num_processes * epochs))
    resume_train_steps = int(np.ceil(args.ensemble_size * 2 / accelerator.num_processes * resume_epoch))
    progress_bar = tqdm(
        range(resume_train_steps + 1, max_train_steps + 1),
        initial=resume_train_steps + 1,
        desc="Steps",
        # Only show the progress bar once on each machine.
        disable=not accelerator.is_main_process,
    )

    if accelerator.is_main_process:
        # Construct a mask for border if not pixel attack
        if not pixel_attack:
            grad_mask = torch.ones_like(accelerator.unwrap_model(model).image.data).reshape(B, C, H, W)
            grad_mask[:, :, border:H-border, border:W-border] = 0.0
        # save copy
        ori_x = ori_image.to(accelerator.unwrap_model(model).image.data)
        momentum = resume_momentum.to(accelerator.unwrap_model(model).image.data)

    # start training
    for epoch in range(resume_epoch + 1, epochs + 1):
        if accelerator.is_main_process:
            loss_buffer = []
            vlm_loss_buffer = []
            rag_loss_buffer = []
        # epoch-based iteration
        for step, batch in enumerate(train_dataloader):
            vlm_loss, rag_loss = model(
                vlm_input_ids=batch["vlm_input_ids"],
                vlm_input_attn=batch["vlm_input_attn"],
                vlm_label_ids=batch["vlm_label_ids"],
                clip_input_ids=batch["clip_input_ids"],
                clip_input_attn=batch["clip_input_attn"],
            )
            target_loss = vlm_loss + rag_loss * lambda_r

            # record loss
            vlm_loss_avg = accelerator.gather(vlm_loss.repeat(B)).mean().item()
            rag_loss_avg = accelerator.gather(rag_loss.repeat(B)).mean().item()
            loss_avg = accelerator.gather(target_loss.repeat(B)).mean().item()

            if accelerator.is_main_process:
                loss_buffer.append(loss_avg)
                vlm_loss_buffer.append(vlm_loss_avg)
                rag_loss_buffer.append(rag_loss_avg)

            # backward
            accelerator.backward(target_loss)

            # logs
            logs = {"VLM_loss": vlm_loss_avg, "RAG_loss": rag_loss_avg, "Overall_loss": loss_avg}
            progress_bar.update(1)
            progress_bar.set_postfix(**logs)
            
            # update parameters in the main process
            if accelerator.is_main_process:              
                data = accelerator.unwrap_model(model).image.data.reshape(B, C, H, W)
                grad = accelerator.unwrap_model(model).image.grad.reshape(B, C, H, W)
                
                # whether to enable pixel attack
                if not pixel_attack:
                    grad = grad * grad_mask
                momentum = mu * momentum - grad / torch.norm(grad.reshape(B, -1), p=1, dim=1).view(B, 1, 1, 1)
                
                data = data + eta * momentum.sign()
                
                if unconstrained:
                    data = data.clamp(0, 1)
                else:
                    data = clamp(data, ori_x, epsilon)

                accelerator.unwrap_model(model).image.data = data.reshape(-1)
                accelerator.unwrap_model(model).zero_grad()
            
        # logging
        if accelerator.is_main_process:
            message = f"[{epoch}/{epochs}] Accumulated vlm loss: {sum(vlm_loss_buffer)/len(vlm_loss_buffer)}, rag loss: {sum(rag_loss_buffer)/len(rag_loss_buffer)}, total loss: {sum(loss_buffer)/len(loss_buffer)}"
            with open(train_log, 'a') as f:
                f.write(message + "\n")
            print(message)
                
        # save image
        if epoch % args.save_epoch == 0:
            if accelerator.is_main_process:
                print('######### Save image and momentum - Epoch = %d ##########' % epoch)
                # fetch model and image
                x = accelerator.unwrap_model(model).image.data.reshape(B, C, H, W)
                adv_img_prompt = x.detach().cpu().squeeze(0)
                save_image(adv_img_prompt, '%s/epoch_%d_Image.png' % (os.path.join(save_dir, "images"), epoch))
                # save momentum
                np.save('%s/epoch_%d_momentum.npy' % (os.path.join(save_dir, "momentum"), epoch), momentum.detach().cpu().to(torch.float32).numpy())

# create dataset
class AttackDataset(Dataset):
    def __init__(self, prompts, queries, target, vlm_processor, clip_processor):
        self.prompts = prompts
        self.queries = queries
        self.target = target
        self.vlm_processor = vlm_processor
        self.clip_processor = clip_processor
        assert len(self.prompts) == len(self.queries)
    
    def __len__(self):
        return len(self.prompts)
        
    def __getitem__(self, index):
        # prepare inputs
        prompt, query, target = self.prompts[index], self.queries[index], self.target
        vlm_inputs = self.vlm_processor(prompt, return_tensors='pt')
        prompt_ids = vlm_inputs.input_ids
        prompt_attn = vlm_inputs.attention_mask
        prompt_lens = prompt_ids.shape[1]

        target_inputs = self.vlm_processor(target, return_tensors='pt')
        target_ids = target_inputs.input_ids[:, 1:]        # remove bos
        target_attn = target_inputs.attention_mask[:, 1:]  # remove bos

        # prepare labels
        context_mask = torch.full([1, prompt_lens+24*24-1], -100).to(prompt_ids)  # padding token id -100, 24*24 refers to the number of image tokens, minus image default token (placeholder token)
        vlm_input_ids = torch.cat([prompt_ids, target_ids], dim=1)
        vlm_input_attn = torch.cat([prompt_attn, torch.ones(1, 24*24-1).to(prompt_ids), target_attn], dim=1)
        vlm_label_ids = torch.cat([context_mask, target_ids], dim=1)

        # prepare clip input
        clip_inputs = self.clip_processor.tokenizer(query, padding="max_length", max_length=77, truncation=True, return_tensors="pt")
        clip_input_ids = clip_inputs.input_ids
        clip_input_attn = clip_inputs.attention_mask
        return {
            "vlm_input_ids": vlm_input_ids,
            "vlm_input_attn": vlm_input_attn,
            "vlm_label_ids": vlm_label_ids,
            "clip_input_ids": clip_input_ids,
            "clip_input_attn": clip_input_attn,
        }

class WrapAdvModel(torch.nn.Module):

    def __init__(self, args, image, 
                 vlm, vlm_transform,
                 clip_model, clip_transform):
        super(WrapAdvModel, self).__init__()

        self.image = torch.nn.Parameter(image) # truly optimized parameters
        
        self.vlm = vlm
        self.vlm_transform = vlm_transform
        self.clip_model = clip_model
        self.clip_transform = clip_transform

        # add input augmentation during forward
        self.upper_random_resize = args.upper_random_resize
        self.lower_random_resize = args.lower_random_resize
        self.enable_random_resize = args.enable_random_resize

        self.prob_random_jpeg = args.prob_random_jpeg
        self.prob_random_flip = args.prob_random_flip
        self.aug_random_flip = torchvision.transforms.RandomHorizontalFlip(p=self.prob_random_flip)

        if self.enable_random_resize:
            print(f"Enable random resize as input augmentation. The upper random size is {self.upper_random_resize}, and the lower random size is {self.lower_random_resize}.")
        
        if self.prob_random_flip > 0:
            print(f"Enable horizontal random flip as input augmentation. The probability of random flip is {self.prob_random_flip}.")
        
        if self.prob_random_jpeg > 0:
            self.jpeg_quality_factors = [75]
            print(f"Enable random jpeg compression as input augmentation. The probability of random jpeg compression is {self.prob_random_jpeg}. The quality factors are selected from {self.jpeg_quality_factors}.")
        
    def forward(self, 
                vlm_input_ids, 
                vlm_input_attn, 
                vlm_label_ids,
                clip_input_ids,
                clip_input_attn,
               ):
        
        # Step 0: randomly perform augmentation
        image = self.image
        if random.random() < self.prob_random_jpeg:   # enable random jpeg compression
            random_jpeg_index = random.randint(0, len(self.jpeg_quality_factors)-1)
            random_jpeg_quality = self.jpeg_quality_factors[random_jpeg_index]
            image = diff_jpeg_coding(image_rgb=image*255, jpeg_quality=torch.tensor([random_jpeg_quality]).to(image.device))
            image = (image / 255).clamp(0, 1)

        image = self.aug_random_flip(image)  # random flip
        
        if self.enable_random_resize:  # enable random resize
            random_resize = random.randint(self.lower_random_resize, self.upper_random_resize)
            aug_random_resize = torchvision.transforms.Resize(random_resize, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True)
            image = aug_random_resize(image)
        
        # Step I: computing ce loss
        pixel_values = self.vlm_transform.preprocess_img(image)
        outputs = self.vlm(
                input_ids=vlm_input_ids,
                attention_mask=vlm_input_attn,
                pixel_values=pixel_values.repeat(vlm_input_ids.shape[0], 1, 1, 1),
                labels=vlm_label_ids,
                )

        vlm_loss = outputs.loss

        # Step II: computing CLIP matching loss
        clip_img_input = self.clip_transform.preprocess_img(image)
        # text_embedding
        text_embeddings_clip = self.clip_model.get_text_features(
                input_ids=clip_input_ids, 
                attention_mask=clip_input_attn,
            )
        text_embeddings_clip = text_embeddings_clip / text_embeddings_clip.norm(p=2, dim=-1, keepdim=True)
        image_embeddings_clip = self.clip_model.get_image_features(clip_img_input)
        image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)
        similarity = text_embeddings_clip @ image_embeddings_clip.T
        rag_loss = - similarity.mean()
        
        return vlm_loss, rag_loss
        

def main():
    # load arguments
    args = parse_args()

    # define data type
    dtype = torch.float32
    if args.dtype == "fp16":
        dtype = torch.float16
    elif args.dtype == "bf16":
        dtype = torch.bfloat16
    
    # prepare experimental folder
    os.makedirs(args.root, exist_ok=True)
    if args.pixel_attack:
        exp_dir = f"{args.root}/{args.div}_diversity_size{args.size}_pixel_epsilon{args.epsilon}_255"
    else:
        exp_dir = f"{args.root}/{args.div}_diversity_size{args.size}_border_width{args.border}"
    os.makedirs(exp_dir, exist_ok=True)
    if args.unconstrained:
        save_dir = os.path.join(exp_dir, f"ensemble_size{args.ensemble_size}_unconstrained_eta{args.eta}_255_lambda_r_{args.lambda_r}_mu{args.mu}")
    else:
        save_dir = os.path.join(exp_dir, f"ensemble_size{args.ensemble_size}_constrained_eta{args.eta}_255_lambda_r_{args.lambda_r}_mu{args.mu}")
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(os.path.join(save_dir, "images"), exist_ok=True)
    os.makedirs(os.path.join(save_dir, "momentum"), exist_ok=True)

    # prepare accelerator
    accelerator = Accelerator(
        mixed_precision=args.dtype,
    )
    print(accelerator.state)
    ####

    # load model
    model_id = args.vlm
    vlm = LlavaForConditionalGeneration.from_pretrained(
        model_id, 
        torch_dtype=dtype,
        low_cpu_mem_usage=True, 
    )
    vlm.eval()
    vlm.requires_grad_(False)
    
    # load preprocessor and construct differential one
    vlm_processor = AutoProcessor.from_pretrained(model_id)
    vlm_transform = ProcessorGradientFlow(processor=vlm_processor.image_processor)

    # load CLIP model for RAG
    clip_model = CLIPModel.from_pretrained(args.rag, torch_dtype=dtype)
    clip_processor = CLIPProcessor.from_pretrained(args.rag)
    clip_model.eval()
    clip_model.requires_grad_(False)
    clip_transform = ProcessorGradientFlow(processor=clip_processor.image_processor)
    
    # load image
    image_path = "demo.png"
    raw_image = Image.open(image_path)
    # resize to specific size
    raw_image = raw_image.resize((args.size, args.size))
    # consider to resume training
    save_image_paths = os.listdir(os.path.join(save_dir, "images"))
    if args.resume and len(save_image_paths) > 0:
        resume_epoch = max([int(save_image_path.split("_")[1]) for save_image_path in save_image_paths])
        resume_image_path = os.path.join(save_dir, "images", f"epoch_{resume_epoch}_Image.png")
        resume_image = Image.open(resume_image_path)
        image = torchvision.transforms.ToTensor()(resume_image.convert('RGB')).unsqueeze(dim=0).to(dtype)
        ori_image = torchvision.transforms.ToTensor()(raw_image.convert('RGB')).unsqueeze(dim=0).to(dtype)
        # load momentum
        resume_momentum = torch.from_numpy(np.load(os.path.join(save_dir, "momentum", f"epoch_{resume_epoch}_momentum.npy")))
    else:
        resume_epoch = 0
        image = torchvision.transforms.ToTensor()(raw_image.convert('RGB')).unsqueeze(dim=0).to(dtype)
        ori_image = image.clone()
        # load momentum
        resume_momentum = torch.zeros_like(image)

    message1 = f"Load image from {image_path}"

    # load target
    target_path = "demo.txt"
    with open(target_path, "r") as f:
        lines = f.readlines()
    targets = [line.split('\n')[0] for line in lines if len(line) > 0]
    target = targets[0] + "</s>"
    message2 = f"Use string {target} as target"
    

    # wrap up model
    #####
    model = WrapAdvModel(args, image, vlm, vlm_transform, clip_model, clip_transform)
    #####
    
    # prepare dataset and dataloader

    # load prompts and queries        
    rows = []
    with open(f"simulation_{args.div}.csv", 'r') as file:
        csvreader = csv.reader(file)
        header = next(csvreader)
        for row in csvreader:
            rows.append(row)
    queries = [row[5] for row in rows]
    active_prompts = [row[6] for row in rows]
    passive_prompts = [row[8] for row in rows]
    message3 = f"Load data from simulation_{args.div}.csv"
    
    train_log = os.path.join(save_dir, "train.log")
    if accelerator.is_main_process:
        print(message1)
        print(message2)
        print(message3)
        # train log
        with open(train_log, 'a') as f:
            f.write(str(args) + "\n")  # write into configs
            f.write(message1 + "\n")
            f.write(message2 + "\n")
            f.write(message3 + "\n")
        
    
    # set seed
    set_seeds(seed=args.seed)

    # split train and test data
    data = list(zip(active_prompts, passive_prompts, queries))
    random.shuffle(data)
    active_prompts_shuffle, passive_prompts_shuffle, queries_shuffle = zip(*data)
    train_queries, test_queries = queries_shuffle[:args.ensemble_size],  queries_shuffle[args.ensemble_size:]
    train_active_prompts, test_active_prompts = active_prompts_shuffle[:args.ensemble_size], active_prompts_shuffle[args.ensemble_size:]
    train_passive_prompts, test_passive_prompts = passive_prompts_shuffle[:args.ensemble_size], passive_prompts_shuffle[args.ensemble_size:]
    
    # combine active and passive prompts
    train_prompts = train_active_prompts + train_passive_prompts
    test_prompts = test_active_prompts + test_passive_prompts
    train_queries = train_queries + train_queries # double it

    train_dataset = AttackDataset(prompts=train_prompts, queries=train_queries, target=target, vlm_processor=vlm_processor, clip_processor=clip_processor)

    def collate_fn(examples):
        # obtain the maximum length
        max_length_vlm_input = max([example["vlm_input_ids"].shape[1] for example in examples])
        max_length_vlm_label = max([example["vlm_label_ids"].shape[1] for example in examples])
        max_length_clip_input = max([example["clip_input_ids"].shape[1] for example in examples])
        
        # padding for vlm (left padding)
        vlm_input_ids = torch.cat([
            torch.cat([torch.full([1, max_length_vlm_input-example["vlm_input_ids"].shape[1]], 32001).to(example["vlm_input_ids"]), example["vlm_input_ids"]], dim=1) 
            for example in examples
        ], dim=0)
        vlm_input_attn = torch.cat([
            torch.cat([torch.full([1, max_length_vlm_label-example["vlm_input_attn"].shape[1]], 0).to(example["vlm_input_attn"]), example["vlm_input_attn"]], dim=1) 
            for example in examples
        ], dim=0)
        vlm_label_ids = torch.cat([
            torch.cat([torch.full([1, max_length_vlm_label-example["vlm_label_ids"].shape[1]], -100).to(example["vlm_label_ids"]), example["vlm_label_ids"]], dim=1) 
            for example in examples
        ], dim=0)
        
        # padding for clip (right padding)
        clip_input_ids = torch.cat([
            torch.cat([example["clip_input_ids"], torch.full([1, max_length_clip_input-example["clip_input_ids"].shape[1]], 49407).to(example["clip_input_ids"])], dim=1) 
            for example in examples
        ], dim=0)
        clip_input_attn = torch.cat([
            torch.cat([example["clip_input_attn"], torch.full([1, max_length_clip_input-example["clip_input_attn"].shape[1]], 0).to(example["clip_input_attn"])], dim=1) 
            for example in examples
        ], dim=0)

        return {
            "vlm_input_ids": vlm_input_ids,
            "vlm_input_attn": vlm_input_attn,
            "vlm_label_ids": vlm_label_ids,
            "clip_input_ids": clip_input_ids,
            "clip_input_attn": clip_input_attn,
        }
    
    train_dataloader = DataLoader(
        train_dataset,
        shuffle=False,
        collate_fn=collate_fn,
        batch_size=1,
        num_workers=0,
    )
        
    model, train_dataloader = accelerator.prepare(model, train_dataloader)

    # Start Attack using MI-FGSM
    AttackMI(args, 
             ori_image,
             resume_epoch,
             resume_momentum,
             accelerator, 
             model, 
             train_dataloader, 
             save_dir,
             train_log)


if __name__ == "__main__":
    main()